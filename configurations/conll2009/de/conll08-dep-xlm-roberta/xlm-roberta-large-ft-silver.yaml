# Customize every aspect of training via flags
trainer:

  # Accumulates grads every k batches or as set up in the dict.
  # Default: ``None``. (type: Union[int, Dict[int, int], null], default: null)
  accumulate_grad_batches: 1

  # Logger setup
  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: lightning_logs
      name: conll09-de-dep-silver-conll08-xlm-roberta-large


# <class 'srl.model.srl_parser.SrlParser'>
model:

  #   (type: bool, default: False)
  language_model_fine_tuning: true

  #   (type: bool, default: False)
  use_sense_candidates: true

  #   (type: bool, default: False)
  use_dependency_trees: true

  #   (type: int, default: 100)
  gnn_hidden_dim: 800

  #   (type: int, default: 100)
  edge_embedding_dim: 60

  #   (type: int, default: 5)
  num_gnn_layers: 1

  #   (type: int, default: 3)
  num_gnn_heads: 4

  #   (type: float, default: 0.2)
  gnn_dropout: 0.25

# <class 'srl.data.dependency_srl_data_module.DependencySrlDataModule'>
data:

  #   (type: str)
  vocabulary_path: data/preprocessed/conll2009/de_conll08/vocabulary.json

  #   (type: Union[str, null], default: null)
  dependency_labels_vocab_path: resources/conll_de_dependency_vocab.json

  #   (type: Union[str, null], default: null)
  train_path: data/preprocessed/conll2009/de_conll08/CoNLL2009_train.json

  #   (type: Union[str, null], default: null)
  dev_path: data/preprocessed/conll2009/de_conll08/CoNLL2009_dev.json

  #   (type: Union[str, null], default: null)
  test_path: data/preprocessed/conll2009/de_conll08/CoNLL2009_test.json

  #   (type: str, default: bert-base-cased)
  language_model_name: xlm-roberta-large

  #   (type: int, default: 32)
  batch_size: 32
