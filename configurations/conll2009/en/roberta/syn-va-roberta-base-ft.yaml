# Customize every aspect of training via flags
trainer:

  # Accumulates grads every k batches or as set up in the dict.
  # Default: ``None``. (type: Union[int, Dict[int, int], null], default: null)
  accumulate_grad_batches: 1

  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: lightning_logs
      name: syn-conll09-va-en-roberta-base


# <class 'srl.model.srl_parser.SrlParser'>
model:

  #   (type: str)
  vocabulary_path: ../data/conll2009/preprocessed/vocabulary.va.json

  #   (type: bool, default: False)
  language_model_fine_tuning: true

  #   (type: bool, default: False)
  use_sense_candidates: true

# <class 'srl.data.dependency_srl_data_module.DependencySrlDataModule'>
data:

  #   (type: str)
  vocabulary_path: ../data/conll2009/preprocessed/vocabulary.va.json

  #   (type: Union[str, null], default: null)
  train_path: ../data/conll2009/preprocessed/CoNLL2009_train.va.json

  #   (type: Union[str, null], default: null)
  dev_path: ../data/conll2009/preprocessed/CoNLL2009_dev.va.json

  #   (type: Union[str, null], default: null)
  test_path: ../data/conll2009/preprocessed/CoNLL2009_test.va.json

  #   (type: str, default: bert-base-cased)
  language_model_name: roberta-base

  #   (type: int, default: 32)
  batch_size: 32
