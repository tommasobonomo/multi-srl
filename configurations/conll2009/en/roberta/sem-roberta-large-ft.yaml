# Customize every aspect of training via flags
trainer:

  # Accumulates grads every k batches or as set up in the dict.
  # Default: ``None``. (type: Union[int, Dict[int, int], null], default: null)
  accumulate_grad_batches: 1

  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: lightning_logs
      name: sem-conll09-en-roberta-large

# <class 'srl.model.srl_parser.SrlParser'>
model:

  #   (type: str)
  vocabulary_path: ../data/conll2009/preprocessed_trankit/vocabulary.json

  #   (type: bool, default: False)
  language_model_fine_tuning: true

  #   (type: bool, default: False)
  use_sense_candidates: true

  # #   (type: float, default: 0.001)
  # learning_rate: 1.0e-03

  # #   (type: float, default: 0.0001)
  # weight_decay: 1.0e-02

  # #   (type: float, default: 5e-05)
  # language_model_learning_rate: 1.0e-05

  # #   (type: float, default: 0.01)
  # language_model_weight_decay: 1.0e-02

# <class 'srl.data.dependency_srl_data_module.DependencySrlDataModule'>
data:

  #   (type: str)
  vocabulary_path: ../data/conll2009/preprocessed_trankit/vocabulary.json

  #   (type: Union[str, null], default: null)
  train_path: ../data/conll2009/preprocessed_trankit/CoNLL2009_train.json

  #   (type: Union[str, null], default: null)
  dev_path: ../data/conll2009/preprocessed_trankit/CoNLL2009_dev.json

  #   (type: Union[str, null], default: null)
  test_path: ../data/conll2009/preprocessed_trankit/CoNLL2009_test.json

  #   (type: str, default: bert-base-cased)
  language_model_name: roberta-large

  #   (type: int, default: 32)
  batch_size: 16
